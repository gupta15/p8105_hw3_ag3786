---
title: "Homework 3"
author: "Aakriti Gupta"
date: 2018-10-14
output: github_document 
---


```{r, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)
```

## **Problem 1**
###Reading and cleaning the dataset

* Importing the BRFSS dataset from p8105.datasets.
* Cleaning variable names
* Focusing on the topic "Overall Health"
* Formatted the data to keep only the useful variables
* Arranged reponses ordered from Excellent to Poor

```{r, data_import, cache=TRUE}
library(p8105.datasets)
data(brfss_smart2010)
```

```{r}
brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  select(-class, -topic, -question, -sample_size, -(confidence_limit_low:geo_location)) %>% 
    mutate(response = fct_relevel(response, c("Excellent", "Very good", "Good", "Fair", "Poor"))) %>%
  arrange(response)
```

### Answering questions using the cleaned dataset

* Question 1: In 2002, which states were observed at 7 locations?

```{r}
brfss %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarise(location_count = n_distinct(locationdesc)) %>%
  filter(location_count == 7) %>% 
  knitr::kable(digits = 1)
```

In the year 2002, **3 states namely Connecticut, Florida and North Carolina**, were observed at 7 locations.

* Question 2: Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

```{r}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 1.0,
  out.width = "90%"
)
theme_set(theme_bw() + theme(legend.position = "right"))

brfss %>%
  group_by(year, locationabbr) %>% 
  summarise(location_count = n_distinct(locationdesc)) %>%
  rename(states = locationabbr) %>% 
  ggplot(aes(x = year, y = location_count)) + geom_smooth(aes(color = states), se = FALSE) + 
  labs(
    title = "Number of locations in each state from 2002 to 2010",
    x = "Year",
    y = "Number of distinct locations",
    caption = "Data from the BRFSS package"
  )
```

* Question 3: Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.



* Question 4: For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))

brfss %>% 
  group_by(year, locationabbr, response) %>% 
  summarise(avg_response = mean(data_value)) %>% 
  rename(states = locationabbr) %>% 
  ggplot(aes(x = year, y = avg_response)) + geom_smooth(aes(color = states), se = FALSE) + 
  facet_grid(. ~ response) +
  labs(
    title = "State-level averages of response categories over time",
    x = "Year",
    y = "Average responses",
    caption = "Data from the BRFSS package"
  )
```

## **Problem 2**
###Reading and cleaning the dataset

```{r, cache=TRUE}
library(p8105.datasets)
data(instacart)

insta = instacart %>% 
  janitor::clean_names()
```

###Summarizing the dataset
We will examine the instacart dataset. Instacart is an online grocery service that allows you to shop online from local stores.This dataset has `r nrow(insta)` observations and `r ncol(insta)` variables. The variables `aisle`, `department`, `eval_set` and `product_name` are of class `character`, whereas  all remaining variables are of class `integer`. 

###Answering questions
* Question 1: How many aisles are there, and which aisles are the most items ordered from?

**There are `r insta %>% pull(aisle) %>% n_distinct()` aisles in this dataset**.

Most items are ordered from **aisle 83 for fresh vegetables, aisle 24 for fresh fruits and aisle 123 for packaged vegetables fruits** as seen below.

```{r}
insta %>%
  group_by(aisle_id, aisle) %>% 
  summarise(item_count = n()) %>% 
  arrange(item_count) %>% 
  tail(3) %>% 
  knitr::kable()
```

* Question 2: Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

```{r}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)
theme_set(theme_bw() + theme(legend.position = "bottom"))

insta %>%
  group_by(aisle_id, aisle) %>% 
  summarise(item_count = n()) %>%
  ggplot(aes(x = aisle_id, y = item_count)) + geom_point(color = "blue") + 
  labs(
    title = "Plot of number of items ordered in each aisle",
    x = "Aisle ID",
    y = "Number of items ordered",
    caption = "Data from the instacart package"
  ) +
   geom_text(aes(label = ifelse(item_count > 35000, as.character(aisle),'')), hjust = 1.1, vjust = 1.1)
```

* Question 3: Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

The table for the most popular items in these aisles is as follows:

```{r}
insta %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarise(product_count = n()) %>%
  filter(product_count == max(product_count)) %>% 
  select(-product_count) %>% 
  knitr::kable()

```

* Question 4: Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

Assuming 0 stands for Sunday, 1 for Monday, and so on...

```{r}
insta %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>%
  mutate(mean_hour = mean(order_hour_of_day)) %>% 
  select(product_name, order_dow, mean_hour) %>%
  distinct() %>%  
  spread(key = order_dow, value = mean_hour) %>% 
  rename(Product = product_name, Sunday = "0", Monday = "1", Tuesday = "2", Wednesday = "3", Thursday = "4", Friday = "5", Saturday = "6") %>% 
  knitr::kable()

```

## **Problem 3**
###Reading the dataset

```{r, cache=TRUE}
library(p8105.datasets)
data(ny_noaa)
```

###Summarizing the dataset
We will examine the ny_noaa dataset. This dataset has `r nrow(ny_noaa)` observations and `r ncol(ny_noaa)` variables. The variables `id`, `tmax`and `tmin` are of class `character`, whereas  `prcp`, `snow` and `snwd` are of class `integer`. 

The variables represent the following:

* id: Weather station ID
* date: Date of observation
* prcp: Precipitation (tenths of mm)
* snow: Snowfall (mm)
* snwd: Snow depth (mm)
* tmax: Maximum temperature (tenths of degrees C)
* tmin: Minimum temperature (tenths of degrees C)

There appear to be significant missing values for the variables `tmax`, `tmin`, `prcp`, `snow` and `snwd`.

```{r}
skimr::skim(ny_noaa)
```

##Cleaning the dataset and answering questions
* Question 1: Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

```{r cache = TRUE}
noaa = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(
    prcp = prcp / 10,
    tmin = as.integer(tmin) / 10,
    tmax = as.integer(tmax) / 10
    ) 
```

* Added variables for year, month, and day based on the date
* Changed units for prcp, tmin and tmax as they were in the tenths of the desired units. Also changed tmin and tmax to integers before dividing by 10.
* *Most commonly observed values for snowfall are `r noaa %>% pull(snow) %>% median(na.rm = TRUE)` because there is no snowfall on most days of the year.*

* Question 2 Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)
theme_set(theme_bw() + theme(legend.position = "bottom"))

noaa %>% 
  group_by(year, id) %>% 
  filter(month %in% c("01", "07")) %>% 
  mutate(mean_tmax = mean(tmax, na.rm = TRUE), month = recode(month, "01" = "January", "07" = "July")) %>% 
  select(year, id, month, mean_tmax) %>% 
  distinct() %>% 
  ggplot(aes(x = year, y = mean_tmax)) + geom_point() + 
  facet_grid(. ~ month) + 
  labs(
    title = "Plot of average max temperature in January and in July in each station across years",
    x = "Years",
    y = "Average maximum temperatures (Celsius)",
    caption = "Data from the ny_noaa package"
  ) 
```

In this 2-panel plot, the average maximum temperatures across majority of the stations are within a 10 degree celsius range. There are outlier stations that have average maximum temperatures much higher or lower than the rest of the stations.

* Question 3. Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)

tmax_tmin_density = noaa %>% 
  ggplot( aes(x = tmax, y = tmin)) + stat_density_2d(aes(fill = ..level..), geom = "polygon", colour = "white")

snow_year_ridges = noaa %>% 
  filter(snow %in% 1:99) %>% 
  ggplot(aes(x = snow, y = year)) + geom_density_ridges(scale = 0.85)

tmax_tmin_density + snow_year_ridges

```

For the plot for tmax versus tmin, given the large dataset, I used density_2d function with contouring to get a better sense of the distribution.

For the plot of snowfall by year, I created a density_ridges plot.
